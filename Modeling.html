<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>Modeling</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>


</head>

<body>

<div><pre><code class="language-python">import pandas as pd
import numpy as np
from sklearn.utils import resample
from sklearn.cross_decomposition import PLSRegression
from sklearn.linear_model import Lasso, RidgeClassifier, LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score, confusion_matrix
from xgboost import XGBClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, Flatten, MaxPooling1D, LSTM, GRU
from sklearn.base import BaseEstimator, ClassifierMixin
from tensorflow.keras.optimizers import Adam</code></pre></div>

<div><pre><code class="language-python"># Define sensitivity and specificity
def sensitivity(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tp / (tp + fn)

def specificity(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return tn / (tn + fp)</code></pre></div>

<h3 id="toc_0">Load data</h3>

<div><pre><code class="language-python">df = pd.read_csv(&#39;./JM006_0901_whole.csv&#39;)
df = df.dropna(subset=[&#39;milkweightlbs&#39;])
df = df.dropna(subset=[&#39;cells&#39;])
df = df.dropna(subset=[&#39;conductivity&#39;])
df_f = df[df[&#39;disease&#39;] == 0]
# df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] &gt; 14] 
# df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] &lt;= 14][df[&#39;disease_in&#39;] &gt;= 11] 
df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] &lt;= 10][df[&#39;disease_in&#39;] &gt;= 8] 
# df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] &lt;= 7][df[&#39;disease_in&#39;] &gt;= 6] 
# df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] &lt;= 5][df[&#39;disease_in&#39;] &gt;= 4] 
# df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] == 3] 
# df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] == 2] 
# df_d = df[df[&#39;disease&#39;] == 1][df[&#39;disease_in&#39;] == 1] 
df = pd.concat([df_f, df_d])
df = df.reset_index(drop=True)
X = np.hstack((df.iloc[:,1:488].values, df[&#39;milkweightlbs_sca&#39;].values.reshape(-1, 1), df[&#39;cells_sca&#39;].values.reshape(-1, 1), df[&#39;conductivity_sca&#39;].values.reshape(-1, 1), df[&#39;parity_sca&#39;].values.reshape(-1, 1)))
y = df[&#39;disease&#39;]</code></pre></div>

<h3 id="toc_1">ML models</h3>

<p>PLS-DA</p>

<div><pre><code class="language-python">plsda = PLSRegression(n_components=2)

cv = StratifiedKFold(n_splits=10, shuffle=True)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_majority = X_train[y_train == 0]
    X_train_minority = X_train[y_train == 1]
    y_train_majority = y_train[y_train == 0]
    
    X_train_majority_downsampled = resample(X_train_majority, 
                                            replace=False,
                                            n_samples=len(X_train_minority),
                                            random_state=42)
    y_train_downsampled = np.array([0]*len(X_train_minority) + [1]*len(X_train_minority))
    
    X_train_downsampled = np.vstack([X_train_majority_downsampled, X_train_minority])
    
    # fit PLSRegression
    plsda.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = plsda.predict(X_test)
    
    # convert continuous values to class labels via a threshold
    y_pred = [1 if x &gt;= 0.5 else 0 for x in y_pred]
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&#39;Average sensitivity: {np.mean(sensitivities)}&#39;)
print(f&#39;Average specificity: {np.mean(specificities)}&#39;)
print(f&#39;Average accuracy: {np.mean(accuracies)}&#39;)

r_pls_da = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;PLS-DA&#39;})
r_pls_da</code></pre></div>

<div><pre><code class="language-none">Average sensitivity: 0.4197802197802198
Average specificity: 0.7258239654610622
Average accuracy: 0.7176536076120035</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.357143</td>
      <td>0.725806</td>
      <td>0.715686</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.285714</td>
      <td>0.731855</td>
      <td>0.719608</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.571429</td>
      <td>0.711694</td>
      <td>0.707843</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.615385</td>
      <td>0.737903</td>
      <td>0.734774</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.307692</td>
      <td>0.725806</td>
      <td>0.715128</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.384615</td>
      <td>0.770161</td>
      <td>0.760314</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.461538</td>
      <td>0.768145</td>
      <td>0.760314</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.500000</td>
      <td>0.703030</td>
      <td>0.697446</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.285714</td>
      <td>0.707071</td>
      <td>0.695481</td>
      <td>PLS-DA</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.428571</td>
      <td>0.676768</td>
      <td>0.669941</td>
      <td>PLS-DA</td>
    </tr>
  </tbody>
</table>
</div>

<p>Ridge regression</p>

<div><pre><code class="language-python">ridge = RidgeClassifier()

cv = StratifiedKFold(n_splits=10, shuffle=True)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_majority = X_train[y_train == 0]
    X_train_minority = X_train[y_train == 1]
    y_train_majority = y_train[y_train == 0]
    
    X_train_majority_downsampled = resample(X_train_majority,
                                            replace=False,
                                            n_samples=len(X_train_minority),
                                            random_state=42)
    
    y_train_downsampled = np.array([0]*len(X_train_minority) + [1]*len(X_train_minority))
    
    X_train_downsampled = np.vstack([X_train_majority_downsampled, X_train_minority])
    
    # fit Ridge
    ridge.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = ridge.predict(X_test)
    
    # convert continuous values to class labels via a threshold
    y_pred = [1 if x &gt;= 0.5 else 0 for x in y_pred]
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&#39;Average sensitivity: {np.mean(sensitivities)}&#39;)
print(f&#39;Average specificity: {np.mean(specificities)}&#39;)
print(f&#39;Average accuracy: {np.mean(accuracies)}&#39;)

r_ridge = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;Ridge&#39;})
r_ridge</code></pre></div>

<div><pre><code class="language-none">Average sensitivity: 0.4758241758241758
Average specificity: 0.7403755294884328
Average accuracy: 0.7333730112870296</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.571429</td>
      <td>0.691532</td>
      <td>0.688235</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.428571</td>
      <td>0.713710</td>
      <td>0.705882</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.500000</td>
      <td>0.743952</td>
      <td>0.737255</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.307692</td>
      <td>0.788306</td>
      <td>0.776031</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.384615</td>
      <td>0.766129</td>
      <td>0.756385</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.461538</td>
      <td>0.685484</td>
      <td>0.679764</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.461538</td>
      <td>0.752016</td>
      <td>0.744597</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.571429</td>
      <td>0.777778</td>
      <td>0.772102</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.500000</td>
      <td>0.713131</td>
      <td>0.707269</td>
      <td>Ridge</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.571429</td>
      <td>0.771717</td>
      <td>0.766208</td>
      <td>Ridge</td>
    </tr>
  </tbody>
</table>
</div>

<p>Lasso regression</p>

<div><pre><code class="language-python">lasso = Lasso(alpha=0.2) 

cv = StratifiedKFold(n_splits=10, shuffle=True)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_majority = X_train[y_train == 0]
    X_train_minority = X_train[y_train == 1]
    y_train_majority = y_train[y_train == 0]
    
    X_train_majority_downsampled = resample(X_train_majority,
                                            replace=False,
                                            n_samples=len(X_train_minority),
                                            random_state=42)
    
    y_train_downsampled = np.array([0]*len(X_train_minority) + [1]*len(X_train_minority))
    
    X_train_downsampled = np.vstack([X_train_majority_downsampled, X_train_minority])
    
    # fit Lasso
    lasso.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = lasso.predict(X_test)
    
    # convert continuous values to class labels via a threshold
    y_pred = [1 if x &gt;= 0.5 else 0 for x in y_pred]
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&#39;Average sensitivity: {np.mean(sensitivities)}&#39;)
print(f&#39;Average specificity: {np.mean(specificities)}&#39;)
print(f&#39;Average accuracy: {np.mean(accuracies)}&#39;)

r_lasso = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;Lasso&#39;})
r_lasso</code></pre></div>

<div><pre><code class="language-none">Average sensitivity: 0.44230769230769224
Average specificity: 0.7548985826001955
Average accuracy: 0.7465017912862592</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.428571</td>
      <td>0.794355</td>
      <td>0.784314</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.142857</td>
      <td>0.774194</td>
      <td>0.756863</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.500000</td>
      <td>0.772177</td>
      <td>0.764706</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.307692</td>
      <td>0.733871</td>
      <td>0.722986</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.461538</td>
      <td>0.703629</td>
      <td>0.697446</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.538462</td>
      <td>0.709677</td>
      <td>0.705305</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.615385</td>
      <td>0.764113</td>
      <td>0.760314</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.642857</td>
      <td>0.763636</td>
      <td>0.760314</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.428571</td>
      <td>0.783838</td>
      <td>0.774067</td>
      <td>Lasso</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.357143</td>
      <td>0.749495</td>
      <td>0.738703</td>
      <td>Lasso</td>
    </tr>
  </tbody>
</table>
</div>

<p>Elastic Net</p>

<div><pre><code class="language-python">en = LogisticRegression(penalty=&#39;elasticnet&#39;, solver=&#39;saga&#39;, l1_ratio=0.5)

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_majority = X_train[y_train == 0]
    X_train_minority = X_train[y_train == 1]
    y_train_majority = y_train[y_train == 0]
    
    X_train_majority_downsampled = resample(X_train_majority, 
                                            replace=False,
                                            n_samples=len(X_train_minority),
                                            random_state=42)
    
    y_train_downsampled = np.array([0]*len(X_train_minority) + [1]*len(X_train_minority))
    
    X_train_downsampled = np.vstack([X_train_majority_downsampled, X_train_minority])
    
    # fit Elastic Net
    en.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = en.predict(X_test)
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&#39;Average sensitivity: {np.mean(sensitivities)}&#39;)
print(f&#39;Average specificity: {np.mean(specificities)}&#39;)
print(f&#39;Average accuracy: {np.mean(accuracies)}&#39;)

r_Elastic_Net = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;EN&#39;})
r_Elastic_Net</code></pre></div>

<div><pre><code class="language-none">Average sensitivity: 0.47087912087912087
Average specificity: 0.7193756109481917
Average accuracy: 0.7127443275935129</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.285714</td>
      <td>0.745968</td>
      <td>0.733333</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.357143</td>
      <td>0.731855</td>
      <td>0.721569</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.642857</td>
      <td>0.677419</td>
      <td>0.676471</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.307692</td>
      <td>0.802419</td>
      <td>0.789784</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.461538</td>
      <td>0.707661</td>
      <td>0.701375</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.461538</td>
      <td>0.729839</td>
      <td>0.722986</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.692308</td>
      <td>0.695565</td>
      <td>0.695481</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.571429</td>
      <td>0.678788</td>
      <td>0.675835</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.357143</td>
      <td>0.652525</td>
      <td>0.644401</td>
      <td>EN</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.571429</td>
      <td>0.771717</td>
      <td>0.766208</td>
      <td>EN</td>
    </tr>
  </tbody>
</table>
</div>

<p>Random forest</p>

<div><pre><code class="language-python">rf = RandomForestClassifier(max_depth=10, n_estimators=100, random_state=42, class_weight=&#39;balanced&#39;)

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_1 = X_train[y_train == 1]
    X_train_0 = X_train[y_train == 0]
    y_train_1 = y_train[y_train == 1]
    y_train_0 = y_train[y_train == 0]

    X_train_0_downsampled = resample(X_train_0,
                                     replace=False,
                                     n_samples=len(X_train_1),
                                     random_state=42)
    
    y_train_0_downsampled = np.zeros(len(X_train_1))

    X_train_downsampled = np.vstack([X_train_0_downsampled, X_train_1])
    y_train_downsampled = np.hstack([y_train_0_downsampled, y_train_1])
    
    # fit Random forest
    rf.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = rf.predict(X_test)
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&quot;Average Sensitivity: {np.mean(sensitivities)}&quot;)
print(f&quot;Average Specificity: {np.mean(specificities)}&quot;)
print(f&quot;Average Accuracy: {np.mean(accuracies)}&quot;)

r_RF = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;RF&#39;})
r_RF</code></pre></div>

<div><pre><code class="language-none">Average Sensitivity: 0.6093406593406593
Average Specificity: 0.6090501792114694
Average Accuracy: 0.6090708424823761</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.857143</td>
      <td>0.582661</td>
      <td>0.590196</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.357143</td>
      <td>0.647177</td>
      <td>0.639216</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.642857</td>
      <td>0.598790</td>
      <td>0.600000</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.692308</td>
      <td>0.554435</td>
      <td>0.557957</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.384615</td>
      <td>0.622984</td>
      <td>0.616896</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.538462</td>
      <td>0.643145</td>
      <td>0.640472</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.692308</td>
      <td>0.552419</td>
      <td>0.555992</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.500000</td>
      <td>0.626263</td>
      <td>0.622790</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.500000</td>
      <td>0.616162</td>
      <td>0.612967</td>
      <td>RF</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.928571</td>
      <td>0.646465</td>
      <td>0.654224</td>
      <td>RF</td>
    </tr>
  </tbody>
</table>
</div>

<p>XGBoost</p>

<div><pre><code class="language-python">xgboost = XGBClassifier(max_depth=10, n_estimators=100, use_label_encoder=False, eval_metric=&#39;logloss&#39;, learning_rate=0.01)

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_1 = X_train[y_train == 1]
    X_train_0 = X_train[y_train == 0]
    y_train_1 = y_train[y_train == 1]
    y_train_0 = y_train[y_train == 0]

    X_train_0_downsampled = resample(X_train_0,
                                     replace=False,
                                     n_samples=len(X_train_1),
                                     random_state=42)
    
    y_train_0_downsampled = np.zeros(len(X_train_1))

    X_train_downsampled = np.vstack([X_train_0_downsampled, X_train_1])
    y_train_downsampled = np.hstack([y_train_0_downsampled, y_train_1])
    
    # fit XGBoost
    xgboost.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = xgboost.predict(X_test)
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&quot;Average Sensitivity: {np.mean(sensitivities)}&quot;)
print(f&quot;Average Specificity: {np.mean(specificities)}&quot;)
print(f&quot;Average Accuracy: {np.mean(accuracies)}&quot;)

r_XGBoost = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;XGBoost&#39;})
r_XGBoost</code></pre></div>

<div><pre><code class="language-none">Average Sensitivity: 0.6087912087912087
Average Specificity: 0.6080335614206582
Average Accuracy: 0.6080808197542279</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.642857</td>
      <td>0.578629</td>
      <td>0.580392</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.285714</td>
      <td>0.637097</td>
      <td>0.627451</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.714286</td>
      <td>0.659274</td>
      <td>0.660784</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.615385</td>
      <td>0.627016</td>
      <td>0.626719</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.461538</td>
      <td>0.546371</td>
      <td>0.544204</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.538462</td>
      <td>0.604839</td>
      <td>0.603143</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.615385</td>
      <td>0.580645</td>
      <td>0.581532</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.642857</td>
      <td>0.630303</td>
      <td>0.630648</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.785714</td>
      <td>0.597980</td>
      <td>0.603143</td>
      <td>XGBoost</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.785714</td>
      <td>0.618182</td>
      <td>0.622790</td>
      <td>XGBoost</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="toc_2">MLP</h3>

<div><pre><code class="language-python">class MLPClassifierCustom(BaseEstimator, ClassifierMixin):
    def __init__(self, input_shape, hidden_layer_sizes=(100,), activation=&#39;relu&#39;, epochs=100, batch_size=32):
        self.input_shape = input_shape 
        self.hidden_layer_sizes = hidden_layer_sizes
        self.activation = activation
        self.epochs = epochs
        self.batch_size = batch_size
        self.model = self._build_model()
        
    def _build_model(self):
        model = Sequential()
        model.add(Dense(self.hidden_layer_sizes[0], activation=self.activation, input_shape=(self.input_shape,)))
        
        for layer_size in self.hidden_layer_sizes[1:]:
            model.add(Dense(layer_size, activation=self.activation))
            
        model.add(Dense(1, activation=&#39;sigmoid&#39;))
        model.compile(optimizer=Adam(learning_rate=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
        return model
    
    def fit(self, X, y):
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)
        return self
    
    def predict_proba(self, X):
        return self.model.predict(X)
    
    def predict(self, X):
        y_pred = self.model.predict(X)
        return (y_pred &gt; 0.5).astype(int).flatten()</code></pre></div>

<div><pre><code class="language-python">mlp = MLPClassifierCustom(input_shape=X.shape[1], hidden_layer_sizes=(64, 128, 64), epochs=200) 

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_1 = X_train[y_train == 1]
    X_train_0 = X_train[y_train == 0]
    y_train_1 = y_train[y_train == 1]
    y_train_0 = y_train[y_train == 0]

    X_train_0_downsampled = resample(X_train_0,
                                     replace=False,
                                     n_samples=len(X_train_1),
                                     random_state=42)
    
    y_train_0_downsampled = np.zeros(len(X_train_1))

    X_train_downsampled = np.vstack([X_train_0_downsampled, X_train_1])
    y_train_downsampled = np.hstack([y_train_0_downsampled, y_train_1])
    
    # fit MLP
    mlp.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = mlp.predict(X_test)
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&quot;Average Sensitivity: {np.mean(sensitivities)}&quot;)
print(f&quot;Average Specificity: {np.mean(specificities)}&quot;)
print(f&quot;Average Accuracy: {np.mean(accuracies)}&quot;)

r_ANN = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;MLP&#39;})
r_ANN</code></pre></div>

<div><pre><code class="language-none">Average Sensitivity: 0.6313186813186814
Average Specificity: 0.7570711143695015
Average Accuracy: 0.7537224084132671</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.407143</td>
      <td>0.878629</td>
      <td>0.865686</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.478571</td>
      <td>0.610484</td>
      <td>0.606863</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.692857</td>
      <td>0.731452</td>
      <td>0.730392</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.511538</td>
      <td>0.874597</td>
      <td>0.865324</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.434615</td>
      <td>0.810081</td>
      <td>0.800491</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.819231</td>
      <td>0.685081</td>
      <td>0.688507</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.819231</td>
      <td>0.757661</td>
      <td>0.759234</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.692857</td>
      <td>0.680303</td>
      <td>0.680648</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.692857</td>
      <td>0.732828</td>
      <td>0.731729</td>
      <td>MLP</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.764286</td>
      <td>0.809596</td>
      <td>0.808350</td>
      <td>MLP</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="toc_3">CNN</h3>

<div><pre><code class="language-python">class CNNClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, input_shape, epochs=100, batch_size=32):
        self.input_shape = input_shape  
        self.epochs = epochs
        self.batch_size = batch_size
        self.model = self._build_model()
        
    def _build_model(self):
        model = Sequential()
        model.add(Conv1D(8, kernel_size=3, activation=&#39;relu&#39;, input_shape=self.input_shape)) # 16
        model.add(Conv1D(12, kernel_size=3, activation=&#39;relu&#39;)) 
        model.add(Flatten())
        model.add(Dense(16, activation=&#39;relu&#39;)) 
        model.add(Dense(1, activation=&#39;sigmoid&#39;))
        model.compile(optimizer=Adam(learning_rate=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
        return model
    
    def fit(self, X, y):
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)
        return self
    
    def predict_proba(self, X):
        return self.model.predict(X)
    
    def predict(self, X):
        y_pred = self.model.predict(X)
        return (y_pred &gt; 0.5).astype(int).flatten()</code></pre></div>

<div><pre><code class="language-python">X_cnn = X.reshape(X.shape[0], X.shape[1], 1)

cnn = CNNClassifier(input_shape=(X_cnn.shape[1], 1), epochs=200)

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X_cnn, y):
    X_train, X_test = X_cnn[train_index], X_cnn[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_1 = X_train[y_train == 1]
    X_train_0 = X_train[y_train == 0]
    y_train_1 = y_train[y_train == 1]
    y_train_0 = y_train[y_train == 0]

    X_train_0_downsampled = resample(X_train_0,
                                     replace=False,
                                     n_samples=len(X_train_1),
                                     random_state=42)
    
    y_train_0_downsampled = np.zeros(len(X_train_1))

    X_train_downsampled = np.vstack([X_train_0_downsampled, X_train_1])
    y_train_downsampled = np.hstack([y_train_0_downsampled, y_train_1])
    
    # fit CNN
    cnn.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = cnn.predict(X_test)
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&quot;Average Sensitivity: {np.mean(sensitivities)}&quot;)
print(f&quot;Average Specificity: {np.mean(specificities)}&quot;)
print(f&quot;Average Accuracy: {np.mean(accuracies)}&quot;)

r_CNN = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;CNN&#39;})
r_CNN</code></pre></div>

<div><pre><code class="language-none">Average Sensitivity: 0.6032967032967034
Average Specificity: 0.7681805962854351
Average Accuracy: 0.7637181709619014</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.550000</td>
      <td>0.834274</td>
      <td>0.826471</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.264286</td>
      <td>0.836290</td>
      <td>0.820588</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.692857</td>
      <td>0.677016</td>
      <td>0.677451</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.511538</td>
      <td>0.852419</td>
      <td>0.843713</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.588462</td>
      <td>0.658871</td>
      <td>0.657073</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.742308</td>
      <td>0.713306</td>
      <td>0.714047</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.819231</td>
      <td>0.783871</td>
      <td>0.784774</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.692857</td>
      <td>0.763131</td>
      <td>0.761198</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.550000</td>
      <td>0.708586</td>
      <td>0.704224</td>
      <td>CNN</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.621429</td>
      <td>0.854040</td>
      <td>0.847642</td>
      <td>CNN</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="toc_4">LSTM</h3>

<div><pre><code class="language-python">class LSTMClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, input_shape, units=20, epochs=100, batch_size=32):
        self.input_shape = input_shape 
        self.units = units  
        self.epochs = epochs
        self.batch_size = batch_size
        self.model = self._build_model()
        
    def _build_model(self):
        model = Sequential()
        model.add(LSTM(self.units, activation=&#39;relu&#39;, input_shape=self.input_shape))
        model.add(Dense(64, activation=&#39;relu&#39;)) 
        model.add(Dense(1, activation=&#39;sigmoid&#39;))
        model.compile(optimizer=Adam(learning_rate=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
        return model
    
    def fit(self, X, y):
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)
        return self
    
    def predict_proba(self, X):
        return self.model.predict(X)
    
    def predict(self, X):
        y_pred = self.model.predict(X)
        return (y_pred &gt; 0.5).astype(int).flatten()</code></pre></div>

<div><pre><code class="language-python">X_lstm = X.reshape(X.shape[0],X.shape[1], 1)

clf = LSTMClassifier(input_shape=(X_lstm.shape[1], 1), units=40, epochs=200, batch_size=32)

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X_lstm, y):
    X_train, X_test = X_lstm[train_index], X_lstm[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_1 = X_train[y_train == 1]
    X_train_0 = X_train[y_train == 0]
    y_train_1 = y_train[y_train == 1]
    y_train_0 = y_train[y_train == 0]

    X_train_0_downsampled = resample(X_train_0,
                                     replace=False,
                                     n_samples=len(X_train_1),
                                     random_state=42)
    
    y_train_0_downsampled = np.zeros(len(X_train_1))

    X_train_downsampled = np.vstack([X_train_0_downsampled, X_train_1])
    y_train_downsampled = np.hstack([y_train_0_downsampled, y_train_1])
    
    # fit LSTM
    clf.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = clf.predict(X_test)
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&quot;Average Sensitivity: {np.mean(sensitivities)}&quot;)
print(f&quot;Average Specificity: {np.mean(specificities)}&quot;)
print(f&quot;Average Accuracy: {np.mean(accuracies)}&quot;)

r_LSTM = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;LSTM&#39;})
r_LSTM</code></pre></div>

<div><pre><code class="language-none">Average Sensitivity: 0.571978021978022
Average Specificity: 0.7748224177256436
Average Accuracy: 0.7694225509457222</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.478571</td>
      <td>0.771774</td>
      <td>0.763725</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.407143</td>
      <td>0.777823</td>
      <td>0.767647</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.692857</td>
      <td>0.759677</td>
      <td>0.757843</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.511538</td>
      <td>0.745565</td>
      <td>0.739587</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.434615</td>
      <td>0.739516</td>
      <td>0.731729</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.511538</td>
      <td>0.910887</td>
      <td>0.900688</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.819231</td>
      <td>0.773790</td>
      <td>0.774951</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.621429</td>
      <td>0.795455</td>
      <td>0.790668</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.478571</td>
      <td>0.688384</td>
      <td>0.682613</td>
      <td>LSTM</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.764286</td>
      <td>0.785354</td>
      <td>0.784774</td>
      <td>LSTM</td>
    </tr>
  </tbody>
</table>
</div>

<h3 id="toc_5">GRU</h3>

<div><pre><code class="language-python">class GRUClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, input_shape, units=20, epochs=100, batch_size=32):
        self.input_shape = input_shape  
        self.units = units  
        self.epochs = epochs
        self.batch_size = batch_size
        self.model = self._build_model()
        
    def _build_model(self):
        model = Sequential()
        model.add(GRU(self.units, activation=&#39;relu&#39;, input_shape=self.input_shape))
        model.add(Dense(64, activation=&#39;relu&#39;))
        model.add(Dense(1, activation=&#39;sigmoid&#39;))
        model.compile(optimizer=Adam(learning_rate=0.0001), loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;])
        return model
    
    def fit(self, X, y):
        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size, verbose=0)
        return self
    
    def predict_proba(self, X):
        return self.model.predict(X)
    
    def predict(self, X):
        y_pred = self.model.predict(X)
        return (y_pred &gt; 0.5).astype(int).flatten()</code></pre></div>

<div><pre><code class="language-python">X_gru = X.reshape(X.shape[0], X.shape[1], 1)

gru = GRUClassifier(input_shape=(X_gru.shape[1], 1), units=40, epochs=200, batch_size=32)

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

sensitivities = []
specificities = []
accuracies = []

for train_index, test_index in cv.split(X_gru, y):
    X_train, X_test = X_gru[train_index], X_gru[test_index]
    y_train, y_test = y[train_index], y[test_index]
    
    # down sampling
    X_train_1 = X_train[y_train == 1]
    X_train_0 = X_train[y_train == 0]
    y_train_1 = y_train[y_train == 1]
    y_train_0 = y_train[y_train == 0]

    X_train_0_downsampled = resample(X_train_0,
                                     replace=False,
                                     n_samples=len(X_train_1),
                                     random_state=42)
    
    y_train_0_downsampled = np.zeros(len(X_train_1))

    X_train_downsampled = np.vstack([X_train_0_downsampled, X_train_1])
    y_train_downsampled = np.hstack([y_train_0_downsampled, y_train_1])
    
    # fit GRU
    gru.fit(X_train_downsampled, y_train_downsampled)
    
    # predict labels
    y_pred = gru.predict(X_test)
    
    # calculate metrics
    sensitivities.append(sensitivity(y_test, y_pred))
    specificities.append(specificity(y_test, y_pred))
    accuracies.append(accuracy_score(y_test, y_pred))

print(f&quot;Average Sensitivity: {np.mean(sensitivities)}&quot;)
print(f&quot;Average Specificity: {np.mean(specificities)}&quot;)
print(f&quot;Average Accuracy: {np.mean(accuracies)}&quot;)

r_GRU = pd.DataFrame({
    &#39;sen&#39;: sensitivities,
    &#39;spe&#39;: specificities,
    &#39;acc&#39;: accuracies,
    &#39;model&#39;: &#39;GRU&#39;})
r_GRU</code></pre></div>

<div><pre><code class="language-none">Average Sensitivity: 0.6175824175824176
Average Specificity: 0.7524580482241773
Average Accuracy: 0.748793482029354</code></pre></div>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sen</th>
      <th>spe</th>
      <th>acc</th>
      <th>model</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.478571</td>
      <td>0.818145</td>
      <td>0.808824</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.335714</td>
      <td>0.779839</td>
      <td>0.767647</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.764286</td>
      <td>0.713306</td>
      <td>0.714706</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.511538</td>
      <td>0.795968</td>
      <td>0.788703</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.588462</td>
      <td>0.779839</td>
      <td>0.774951</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.742308</td>
      <td>0.675000</td>
      <td>0.676719</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.819231</td>
      <td>0.620565</td>
      <td>0.625639</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.692857</td>
      <td>0.817677</td>
      <td>0.814244</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.550000</td>
      <td>0.773232</td>
      <td>0.767092</td>
      <td>GRU</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.692857</td>
      <td>0.751010</td>
      <td>0.749411</td>
      <td>GRU</td>
    </tr>
  </tbody>
</table>
</div>

<div><pre><code class="language-python"></code></pre></div>




</body>

</html>
